{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import json \n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import UNETR\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "import pickle\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataloader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTrain(Dataset):\n",
    "\n",
    "    def __init__(self, data, annotation):\n",
    "        self.traininputtensor = torch.tensor(data, dtype=torch.float)\n",
    "        self.output = torch.tensor(annotation, dtype=torch.float)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_image = self.traininputtensor[index].unsqueeze(0)  \n",
    "        output_label = self.output[index].unsqueeze(0)  \n",
    "        return input_image, output_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.traininputtensor.size(dim=0)\n",
    "\n",
    "\n",
    "class DataTest(Dataset):\n",
    "\n",
    "    def __init__(self, data, annotation):\n",
    "        self.testinputtensor = torch.tensor(data, dtype=torch.float)\n",
    "        self.output = torch.tensor(annotation, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_image = self.testinputtensor[index].unsqueeze(0) \n",
    "        output_label = self.output[index].unsqueeze(0) \n",
    "        return input_image, output_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.testinputtensor.size(dim=0)\n",
    "\n",
    "\n",
    "def reshape_mask(mask,  gray_step=25, max_gray=255):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cut_data(data, annotation, cut_parameter):\n",
    "    final_set_data = []\n",
    "    final_set_annotation = []\n",
    "    for i in range(len(data), step=cut_parameter):\n",
    "        final_set_data.append(data[i:i+cut_parameter])\n",
    "        final_set_annotation.append(annotation[i:i+cut_parameter])\n",
    "    return [final_set_data, final_set_annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = get_default_device()\n",
    "model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=8,\n",
    "    img_size=(100, 512, 512),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "modelwise = UNETR(   #just_to_copy_weights\n",
    "    in_channels=1,\n",
    "    out_channels=14,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "modelwise.load_state_dict(torch.load(os.path.join(r\"/storage/brno2/home/yauheni\", \"best_metric_model.pth\")))\n",
    "model_state_dict1 = modelwise.state_dict()\n",
    "for name_dst, param_dst in model.named_parameters():\n",
    "    if name_dst in modelwise.state_dict():\n",
    "        param_src = model.state_dict()[name_dst]\n",
    "        if param_src.size() == param_dst.size():\n",
    "            param_dst.data.copy_(param_src.data)\n",
    "        else:\n",
    "            print(f\"Skipping layer {name_dst} due to size mismatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
