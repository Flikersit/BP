{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import json \n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import UNETR\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    ")\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataloader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first 3D image:  (882, 512, 512)\n",
      "Shape of the first 3D mask (882, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                                                                ####################################################\n",
    "                                                                ######## Reading data from my local PC #############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "masks_path = Path(r'C:\\working_space\\pilsen_pigs_2023_cvat_backup\\masks_for_all_organs')\n",
    "data_path = Path(r'C:\\working_space\\pilsen_pigs_2023_cvat_backup\\workspase')\n",
    "\n",
    "sorted_pictures = sorted(\n",
    "        [d.name for d in data_path.iterdir() if d.is_dir() and d.name.isdigit()],\n",
    "        key=lambda x: int(x)\n",
    "    )\n",
    "\n",
    "sorted_masks = sorted(\n",
    "        [d.name for d in masks_path.iterdir() if d.is_dir() and d.name.isdigit()],\n",
    "        key=lambda x: int(x)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "images_3d_full = []\n",
    "for image_3d in sorted_pictures:\n",
    "    images_path = Path(fr'C:\\working_space\\pilsen_pigs_2023_cvat_backup\\workspase\\{image_3d}\\data')\n",
    "    images = []\n",
    "    for image in images_path.iterdir():\n",
    "        img = Image.open(image)\n",
    "        img = img.convert('L')\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "    images_3d_full.append(np.array(images))\n",
    "print(\"Shape of the first 3D image: \", images_3d_full[0].shape)\n",
    "\n",
    "masks_3d_full = []\n",
    "for mask_3d in sorted_masks:\n",
    "    masks_path = Path(fr'C:\\working_space\\pilsen_pigs_2023_cvat_backup\\masks_for_all_organs\\{mask_3d}')\n",
    "    masks = []\n",
    "    for mask in masks_path.iterdir():\n",
    "        img = Image.open(mask)\n",
    "        img = np.array(img)\n",
    "        masks.append(img)\n",
    "    masks_3d_full.append(np.array(masks))\n",
    "print(\"Shape of the first 3D mask\", masks_3d_full[0].shape)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for changed mask:  (8, 100, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "                                                                ########################################################\n",
    "                                                                #Processing data from ny local PC and using dataloaders#\n",
    "\n",
    "\n",
    "class DataTrain(Dataset):\n",
    "\n",
    "    def __init__(self, data, annotation):\n",
    "        self.traininputtensor = torch.tensor(data, dtype=torch.float)\n",
    "        self.output = torch.tensor(annotation, dtype=torch.float)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_image = self.traininputtensor[index].unsqueeze(0)  \n",
    "        output_label = self.output[index].unsqueeze(0)  \n",
    "        return input_image, output_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.traininputtensor.size(dim=0)\n",
    "\n",
    "\n",
    "class DataTest(Dataset):\n",
    "\n",
    "    def __init__(self, data, annotation):\n",
    "        self.testinputtensor = torch.tensor(data, dtype=torch.float)\n",
    "        self.output = torch.tensor(annotation, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_image = self.testinputtensor[index].unsqueeze(0) \n",
    "        output_label = self.output[index].unsqueeze(0) \n",
    "        return input_image, output_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.testinputtensor.size(dim=0)\n",
    "\n",
    "\n",
    "def reshape_mask(mask, num_clases=8, depth=100):\n",
    "    \"\"\"We wanted to convert our 3d masks that have shape (1,D,H,W) to the shape (num_classes,D,H,W) \"\"\"\n",
    "    new_mask = np.zeros((num_clases, depth, 512, 512), dtype=np.uint8)\n",
    "\n",
    "    for z in range(depth):  #iteration over 'z' axis \n",
    "        for y in range(512):    #iteration over 'y' axis\n",
    "            for x in range(512):    #iteration over 'x' axis\n",
    "                if mask[z,x,y] != 0:\n",
    "\n",
    "                    value = mask[x,y]\n",
    "                    trida = value/25\n",
    "                    new_mask[trida, z, x, y] = 1\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "\n",
    "def cut_data(data, z_shape=100):\n",
    "    \"\"\"Metacentrum haven't got enough memory to process one full image, that's why it will be cutted \"\"\"\n",
    "    cutted_data = []\n",
    "\n",
    "    for i in range(0, data.shape[0], z_shape):\n",
    "        new_data = data[i:i+z_shape, :, :]\n",
    "        cutted_data.append(new_data)\n",
    "\n",
    "    #for neural network we must have same dimension\n",
    "    if len(cutted_data[-1]) < z_shape:\n",
    "        cutted_data.pop(-1)\n",
    "\n",
    "    return np.array(cutted_data)\n",
    "\n",
    "\n",
    "#experement with one 3d picture\n",
    "one_3d_picture = images_3d_full[0]\n",
    "one_3d_mask = masks_3d_full[0]\n",
    "\n",
    "cutted_picture = cut_data(one_3d_picture)\n",
    "cutted_mask = cut_data(one_3d_mask)\n",
    "\n",
    "reshaped_mask = reshape_mask(cutted_mask[0])\n",
    "print(\"Shape for changed mask: \", reshaped_mask.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' modelwise.load_state_dict(torch.load(os.path.join(r\"/storage/brno2/home/yauheni\", \"best_metric_model.pth\")))\\nmodel_state_dict1 = modelwise.state_dict()\\nfor name_dst, param_dst in model.named_parameters():\\n    if name_dst in modelwise.state_dict():\\n        param_src = model.state_dict()[name_dst]\\n        if param_src.size() == param_dst.size():\\n            param_dst.data.copy_(param_src.data)\\n        else:\\n            print(f\"Skipping layer {name_dst} due to size mismatch\") '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = get_default_device()\n",
    "\"\"\" model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=8,\n",
    "    img_size=(100, 512, 512),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "modelwise = UNETR(   #just_to_copy_weights\n",
    "    in_channels=1,\n",
    "    out_channels=14,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\"\"\" modelwise.load_state_dict(torch.load(os.path.join(r\"/storage/brno2/home/yauheni\", \"best_metric_model.pth\")))\n",
    "model_state_dict1 = modelwise.state_dict()\n",
    "for name_dst, param_dst in model.named_parameters():\n",
    "    if name_dst in modelwise.state_dict():\n",
    "        param_src = model.state_dict()[name_dst]\n",
    "        if param_src.size() == param_dst.size():\n",
    "            param_dst.data.copy_(param_src.data)\n",
    "        else:\n",
    "            print(f\"Skipping layer {name_dst} due to size mismatch\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 96, 96, 96])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.randn(1, 1, 96, 96, 96) #now we know shape of tensor we want \n",
    "out = modelwise(tensor)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
